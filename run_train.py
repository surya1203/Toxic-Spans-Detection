# -*- coding: utf-8 -*-
"""run_train.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rN2KB39NV3YocNjuo4uyDWw0JdxV5ikT
"""

# pip install -r requirements.txt

# from google.colab import drive
# drive.mount('/content/drive')

"""## Loading Libraries"""

import pandas as pd
import numpy as np
import json
import gc
# import stanza
from tensorflow.keras import *
import tensorflow as tf
from tensorflow.keras import *
import tensorflow.keras.backend as K
from sklearn.metrics import classification_report
from sklearn.model_selection import KFold
from transformers import TFElectraModel,ElectraTokenizer
from keras import callbacks

"""## Load dataset

"""

# Training
train_dataset = pd.read_csv('tsd_train.csv')

train_dataset

# Creating python dictionary from json string for spans
train_dataset['spans'] = train_dataset['spans'].apply(lambda x : json.loads(x))
# train_dataset

# Converting spans to numpy
spans = train_dataset['spans'].to_numpy()
# spans

# Converting texts to numpy
texts = train_dataset['text'].to_numpy()
# texts

"""## Tokenize Using Electra"""

# Tokenizer
tokenizer = ElectraTokenizer.from_pretrained('google/electra-base-discriminator')

"""## Generate inputs and outputs"""

max_length = 400
def create_outputs(texts,spans,max_length,tokenizer):
    outputs = []
    for text,span in zip(texts,spans):
        output = np.zeros(max_length*3,dtype=np.float).reshape((max_length,3))
        tokens = tokenizer.tokenize(text)[:max_length]
        length = 0
        start = True
        for i in range(len(tokens),max_length):
            output[i,0] = 1.0
        for index,token in enumerate(tokens):
            sub = False
            if "##" in token:
                sub = True
                token = token[2:]
            if not start:
                next_index = text[length:].find(token)
                if next_index == 0:
                    sub = True
                length += next_index
            # if length in span and not sub:
            #     output[index,2] = 1.0
            #     output[index,0] = 0.0
            if length in span:
                output[index,2] = 1.0
                output[index,0] = 0.0
            else:
                output[index,1] = 1.0
                output[index,0] = 0.0
            length += len(token)
            start = False
        outputs.append(output)
    return np.array(outputs)

def create_inputs(texts,max_length,tokenizer):
  # Tokenize the input texts
  tokens = tokenizer(texts, max_length=max_length, padding="max_length", return_tensors="tf",truncation=True)
  # Get input length for each text
  input_length = []
  for text in texts:
    input_length.append(min(max_length,len(tokenizer.tokenize(text))))

  # Generate arrays of the tokenized inputs
  # Input IDs
  tokenized_input_ids = np.array(tokens['input_ids'])
  # Attention mask
  tokenized_token_type_ids = np.array(tokens['token_type_ids'])
  # Token type IDs
  tokenized_attention_mask = np.array(tokens['attention_mask'])
  # Input length
  nparray_input_length = np.array(input_length)
  
  inputs = [tokenized_input_ids, tokenized_token_type_ids, tokenized_attention_mask, nparray_input_length]
  return inputs

"""## K-Fold for Validation"""

# # # Approximately split train:validation = 80:20
# # kf = KFold(n_splits=5)
# # train_validation_indices = []
# # for train_index, validation_index in kf.split(texts):
# #   train_validation_indices.append((train_index, validation_index))

# # train_validation_indices

# from sklearn.model_selection import train_test_split
# train_data, validation_data = train_test_split(train_dataset, test_size=0.20, random_state=42, shuffle=True)

outputs = create_outputs(texts,train_dataset['spans'],max_length,tokenizer)
kf = KFold(n_splits=5)
train_validation_indices = []
for train_index,validation_index in kf.split(texts):
    train_validation_indices.append((train_index,validation_index))

"""## Split dataset into training and validation"""

# # train_index, validation_index = train_validation_indices.pop()
# x_train, x_validation = list(train_dataset['text'].to_numpy()), list(validation_dataset['text'].to_numpy())
# y_train = create_outputs(train_dataset['text'].to_numpy(),train_dataset['spans'],max_length,tokenizer)
# y_validation = create_outputs(validation_dataset['text'].to_numpy(),validation_dataset['spans'],max_length,tokenizer)
# # y_train, y_validation = outputs[train_index], outputs[validation_index]
train_index, validation_index = train_validation_indices.pop()
x_train , x_validation = list(texts[train_index]) , list(texts[validation_index])
y_train , y_validation = outputs[train_index] , outputs[validation_index]

"""## Build Model"""

from crf_helper import *

def build_lstm_model(max_input_length,base_model):
    input_ids_layer = layers.Input(shape=(max_input_length,),name="encoder_input_ids",dtype=tf.int32)
    input_type_ids_layer = layers.Input(shape=(max_input_length,),name="encoder_token_type_ids",dtype=tf.int32)
    input_attention_mask_layer = layers.Input(shape=(max_input_length,),name="encoder_attention_mask",dtype=tf.int32)
    input_length = layers.Input(shape=(1,),name="length",dtype=tf.int32)
    base_model.trainable = True
    # a list of varying length with one or several input Tensors IN THE ORDER given in the docstring: 
    # model([input_ids, attention_mask]) or model([input_ids, attention_mask, token_type_ids])
    base_model = base_model(input_ids_layer,token_type_ids=input_type_ids_layer,attention_mask=input_attention_mask_layer,return_dict=True)
    output = layers.LSTM(512,return_sequences=True)(base_model.last_hidden_state)
    output = layers.Dense(3,activation="linear")(output)
    crf = CRFLayer()
    output = crf(inputs=[output,input_length])
    model = models.Model(inputs=[input_ids_layer,input_type_ids_layer,input_attention_mask_layer,input_length],outputs=output)
    model.compile(optimizer=optimizers.Adam(learning_rate=3e-5),loss=crf.loss,metrics=['accuracy'])
    return model

def build_bilstm_model(max_input_length,base_model):
    input_ids_layer = layers.Input(shape=(max_input_length,),name="encoder_input_ids",dtype=tf.int32)
    input_type_ids_layer = layers.Input(shape=(max_input_length,),name="encoder_token_type_ids",dtype=tf.int32)
    input_attention_mask_layer = layers.Input(shape=(max_input_length,),name="encoder_attention_mask",dtype=tf.int32)
    input_length = layers.Input(shape=(1,),name="length",dtype=tf.int32)
    base_model.trainable = True
    # a list of varying length with one or several input Tensors IN THE ORDER given in the docstring: 
    # model([input_ids, attention_mask]) or model([input_ids, attention_mask, token_type_ids])
    base_model = base_model(input_ids_layer,token_type_ids=input_type_ids_layer,attention_mask=input_attention_mask_layer,return_dict=True)
    output = layers.Bidirectional(layers.LSTM(512,return_sequences=True))(base_model.last_hidden_state)
    output = layers.Dense(3,activation="linear")(output)
    crf = CRFLayer()
    output = crf(inputs=[output,input_length])
    model = models.Model(inputs=[input_ids_layer,input_type_ids_layer,input_attention_mask_layer,input_length],outputs=output)
    model.compile(optimizer=optimizers.Adam(learning_rate=3e-5),loss=crf.loss,metrics=['accuracy'])
    return model

gc.collect()
tf.keras.backend.clear_session()
base_model = TFElectraModel.from_pretrained('google/electra-base-discriminator')
model_lstm = build_lstm_model(max_length,base_model)

"""## Fit model"""

train_data = create_inputs(x_train,max_length,tokenizer)
validation_data = create_inputs(x_validation,max_length,tokenizer)

spans_validation = spans[validation_index]
spans_train = spans[train_index]

def NERGetIndicesSingleText(outputs,text,tokenizer):
    outputs = tf.argmax(outputs,axis=-1)
    tokens = tokenizer.tokenize(text)
    index = 0
    indexes = []
    sub = False
    prev = False
    for token,output in zip(tokens,outputs):
        if token[:2] == "##":
            token = token[2:]
            sub = True
        else:
            sub = False
        temp_index = text[index:].find(token)
        temp_start = index+temp_index
        if output == 2 or (sub and prev and output != 0):
            prev = True
            indexes = indexes + list(range(temp_start,temp_start+len(token)))
        else:
            prev = False
        index = temp_start+len(token)
    return np.array(indexes)

def createIndicesForNERModel(predicts,texts,tokenizer):
    outputs = []
    for text,pred in zip(texts,predicts):
         indices = NERGetIndicesSingleText(pred,text,tokenizer)
         outputs.append(indices)
    return outputs

def f1(preds,trues):
    if len(trues) == 0:
        return 1. if len(preds) == 0 else 0.
    if len(preds) == 0:
        return 0.
    predictions_set = set(preds)
    gold_set = set(trues)
    nom = 2 * len(predictions_set.intersection(gold_set))
    denom = len(predictions_set) + len(gold_set)
    return float(nom)/float(denom)

def avg_f1(preds,trues):
    avg_f1_total = 0.0
    for pred,true in zip(preds,trues):
        avg_f1_total += f1(pred,true)
    return avg_f1_total/len(preds)

class F1Metric(callbacks.Callback):
    def __init__(self,inputs,labels,spans,texts,test=True):
        self.inputs = inputs
        self.spans = spans
        self.tokenizer = tokenizer
        self.texts = texts
        self.test = test

    def on_epoch_end(self, epoch, logs={}):
        preds = self.model.predict(self.inputs,verbose=0)
        indices = createIndicesForNERModel(preds,texts,tokenizer)
        f1 = avg_f1(indices,self.spans)
        if self.test:
            print()
            print("test f1 = "+str(f1))
        else:
            print()
            print("train f1 = "+str(f1))

# Model Checkpoitns for local storage
# checkpoint_path = "./training_1/crf/"
# checkpoint_dir = os.path.dirname(checkpoint_path)

# MC = callbacks.ModelCheckpoint(filepath= checkpoint_path, verbose =2, save_weights_only=True )

model_lstm.fit(train_data,y_train,batch_size=16,epochs=2,callbacks=[callbacks.ModelCheckpoint("/content/drive/MyDrive/T2_Checkpoints_Giri_Surya/lstm/checkpoints/lstm",save_weights_only=True)]) # callbacks = [MC]

preds_lstm = model_lstm.predict(validation_data)
indices = createIndicesForNERModel(preds_lstm,x_validation,tokenizer)
f1_toxic = avg_f1(indices,spans_validation)

print("test F1 = %f"%(f1_toxic))

"""## Bidirectional LSTM model"""

gc.collect()
tf.keras.backend.clear_session()
base_model = TFElectraModel.from_pretrained('google/electra-base-discriminator')
model_bilstm = build_bilstm_model(max_length,base_model)

model_bilstm.fit(train_data,y_train,batch_size=8,epochs=2,callbacks=[callbacks.ModelCheckpoint("/content/drive/MyDrive/T2_Checkpoints_Giri_Surya/bilstm/checkpoints/bilstm",save_weights_only=True)])
preds_bilstm = model_bilstm.predict(validation_data)
indices = createIndicesForNERModel(preds_bilstm,x_validation,tokenizer)
f1_toxic = avg_f1(indices,spans_validation)
print("test F1 = %f"%(f1_toxic))

